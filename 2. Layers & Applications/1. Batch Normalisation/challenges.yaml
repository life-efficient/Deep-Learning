- name: Batch Norm in PyTorch
  description: |
    - Add batch norm layers to a network and create a training loop to compare it's performance
      - Does it train with the same learning rate?
      - How does the loss curve compare to without batch norm?
    - Can you integrate batch norm into your model experimentation/evaluation/comparison code you've used previously?
  
- name: Implementing your own Batch Norm layer
  description: |
    - Create a class which inherits from torch.nn.Module
    - Initialise the parameters beta and gamma which this batch norm layer will require
      - Check out the class torch.nn.Parameter. 
        - Why should it be used here?
    - Initialise two variables to store the running mean and the running variance
      - Check out the `register_buffer` method which `torch.nn.Module provides`
        - Why is it appropriate to use here?
    - Pause and reflect on these two different sets of variables which have been created. Make sure you're clear on which will be learnt through gradient descent, and which will be computed manually.
    - Implement the forward pass
      - How are the parameters used to transform the input data?
      - Update the parameters to reflect the running average of the batch-wise mean and std deviation.
        - There is a formula to do this exactly, but it's just as effective to use an exponential average
          - resulting_value = (mu * old_value) + ( (1 - mu) * new_incoming_value )
      - Remember these are not parameters which should be updated by the backward pass, so make sure they dont have requires_grad and are detached from the graph!
        - If you set them up correctly as `torch.nn.Parameters` they shouldn't be in the graph
    - Add this custom batch norm layer to a neural network and check that it works similarly to the pytorch layer
