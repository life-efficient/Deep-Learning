- name: Weight decay but now with a deep model
  description: |
    - Check the pytorch docs and apply weight decay (in the form of L2 regularization) to optimising a neural network on the MNIST dataset
      - Where does this get applied? Hint: this lesson is on optimisers, not loss functions

- name: Exploring the pytorch optimisers
  description: |
    - Load in the MNIST dataset
    - Create your data splits
    - Create your dataloaders from those splits
    - Create a small neural network
    - Perform a grid search over different pytorch optimisers and plot their loss curves on the same graph
      - Remember to reinitialise your network each time you train it
      - Sample a few different random seeds
      - For each seed perform a trail with each optimiser (they should all have the same loss if you test them before the optimisation begins)
      - For the different seeds, the curves for each optimiser should look different
      - Don't forget to try toggling momentum
        - Where does this get applied? Hint: where did you apply weight decay?
      - Optimisation might take a while. Can you spread the task amongst a group?
    - How does this performance compare with other datasets from torchvision?

- name: Your own optimiser
  description: |
    - Create your own implementation of gradient descent as a pytorch class